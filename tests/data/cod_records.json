{
  "aggregations": {
    "collision_energy": {
      "buckets": [
        {
          "doc_count": 14, 
          "key": "3.5TeV"
        }, 
        {
          "doc_count": 1941, 
          "key": "7TeV"
        }, 
        {
          "doc_count": 1591, 
          "key": "8TeV"
        }
      ], 
      "doc_count_error_upper_bound": 0, 
      "sum_other_doc_count": 0
    }, 
    "collision_type": {
      "buckets": [
        {
          "doc_count": 6, 
          "key": "PbPb"
        }, 
        {
          "doc_count": 2047, 
          "key": "pp"
        }
      ], 
      "doc_count_error_upper_bound": 0, 
      "sum_other_doc_count": 0
    }, 
    "experiment": {
      "buckets": [
        {
          "doc_count": 26, 
          "key": "ALICE"
        }, 
        {
          "doc_count": 115, 
          "key": "ATLAS"
        }, 
        {
          "doc_count": 3600, 
          "key": "CMS"
        }, 
        {
          "doc_count": 11, 
          "key": "LHCb"
        }, 
        {
          "doc_count": 835, 
          "key": "OPERA"
        }
      ], 
      "doc_count_error_upper_bound": 0, 
      "sum_other_doc_count": 0
    }, 
    "file_type": {
      "buckets": [
        {
          "doc_count": 3, 
          "key": "C"
        }, 
        {
          "doc_count": 85, 
          "key": "aod"
        }, 
        {
          "doc_count": 681, 
          "key": "aodsim"
        }, 
        {
          "doc_count": 6, 
          "key": "cc"
        }, 
        {
          "doc_count": 847, 
          "key": "csv"
        }, 
        {
          "doc_count": 2, 
          "key": "db"
        }, 
        {
          "doc_count": 1, 
          "key": "docx"
        }, 
        {
          "doc_count": 17, 
          "key": "gz"
        }, 
        {
          "doc_count": 7, 
          "key": "html"
        }, 
        {
          "doc_count": 94, 
          "key": "ig"
        }, 
        {
          "doc_count": 1, 
          "key": "ipynb"
        }, 
        {
          "doc_count": 1, 
          "key": "jpg"
        }, 
        {
          "doc_count": 10, 
          "key": "json"
        }, 
        {
          "doc_count": 1, 
          "key": "m4v"
        }, 
        {
          "doc_count": 2, 
          "key": "ova"
        }, 
        {
          "doc_count": 15, 
          "key": "pdf"
        }, 
        {
          "doc_count": 2, 
          "key": "png"
        }, 
        {
          "doc_count": 923, 
          "key": "py"
        }, 
        {
          "doc_count": 1, 
          "key": "raw"
        }, 
        {
          "doc_count": 848, 
          "key": "root"
        }, 
        {
          "doc_count": 1, 
          "key": "tar"
        }, 
        {
          "doc_count": 1, 
          "key": "tar.gz"
        }, 
        {
          "doc_count": 14, 
          "key": "txt"
        }, 
        {
          "doc_count": 1, 
          "key": "xls"
        }, 
        {
          "doc_count": 6, 
          "key": "xml"
        }, 
        {
          "doc_count": 4, 
          "key": "zip"
        }
      ], 
      "doc_count_error_upper_bound": 0, 
      "sum_other_doc_count": 0
    }, 
    "keywords": {
      "buckets": [
        {
          "doc_count": 13, 
          "key": "education"
        }, 
        {
          "doc_count": 9, 
          "key": "external resource"
        }, 
        {
          "doc_count": 66, 
          "key": "masterclass"
        }, 
        {
          "doc_count": 7, 
          "key": "teaching"
        }
      ], 
      "doc_count_error_upper_bound": 0, 
      "sum_other_doc_count": 0
    }, 
    "topic_category": {
      "buckets": [
        {
          "doc_count": 16, 
          "key": "B-physics"
        }, 
        {
          "doc_count": 65, 
          "key": "BSM Higgs"
        }, 
        {
          "doc_count": 50, 
          "key": "SM Exclusive"
        }, 
        {
          "doc_count": 183, 
          "key": "SM Higgs"
        }, 
        {
          "doc_count": 48, 
          "key": "SM Inclusive"
        }, 
        {
          "doc_count": 19, 
          "key": "SM Systematic Variations"
        }
      ], 
      "doc_count_error_upper_bound": 0, 
      "sum_other_doc_count": 0
    }, 
    "type": {
      "buckets": [
        {
          "doc_count": 1828, 
          "key": "Dataset", 
          "subtype": {
            "buckets": [
              {
                "doc_count": 100, 
                "key": "Collision"
              }, 
              {
                "doc_count": 1004, 
                "key": "Derived"
              }, 
              {
                "doc_count": 723, 
                "key": "Simulated"
              }
            ], 
            "doc_count_error_upper_bound": 0, 
            "sum_other_doc_count": 0
          }
        }, 
        {
          "doc_count": 59, 
          "key": "Documentation", 
          "subtype": {
            "buckets": [
              {
                "doc_count": 9, 
                "key": "About"
              }, 
              {
                "doc_count": 19, 
                "key": "Activities"
              }, 
              {
                "doc_count": 5, 
                "key": "Authors"
              }, 
              {
                "doc_count": 16, 
                "key": "Guide"
              }, 
              {
                "doc_count": 2, 
                "key": "Help"
              }, 
              {
                "doc_count": 4, 
                "key": "Policy"
              }, 
              {
                "doc_count": 1, 
                "key": "Report"
              }
            ], 
            "doc_count_error_upper_bound": 0, 
            "sum_other_doc_count": 0
          }
        }, 
        {
          "doc_count": 19, 
          "key": "Environment", 
          "subtype": {
            "buckets": [
              {
                "doc_count": 5, 
                "key": "Condition"
              }, 
              {
                "doc_count": 11, 
                "key": "VM"
              }, 
              {
                "doc_count": 3, 
                "key": "Validation"
              }
            ], 
            "doc_count_error_upper_bound": 0, 
            "sum_other_doc_count": 0
          }
        }, 
        {
          "doc_count": 22, 
          "key": "Glossary", 
          "subtype": {
            "buckets": [], 
            "doc_count_error_upper_bound": 0, 
            "sum_other_doc_count": 0
          }
        }, 
        {
          "doc_count": 10, 
          "key": "News", 
          "subtype": {
            "buckets": [], 
            "doc_count_error_upper_bound": 0, 
            "sum_other_doc_count": 0
          }
        }, 
        {
          "doc_count": 33, 
          "key": "Software", 
          "subtype": {
            "buckets": [
              {
                "doc_count": 16, 
                "key": "Analysis"
              }, 
              {
                "doc_count": 4, 
                "key": "Framework"
              }, 
              {
                "doc_count": 8, 
                "key": "Tool"
              }, 
              {
                "doc_count": 5, 
                "key": "Validation"
              }
            ], 
            "doc_count_error_upper_bound": 0, 
            "sum_other_doc_count": 0
          }
        }, 
        {
          "doc_count": 2642, 
          "key": "Supplementaries", 
          "subtype": {
            "buckets": [
              {
                "doc_count": 213, 
                "key": "Configuration HLT"
              }, 
              {
                "doc_count": 242, 
                "key": "Configuration LHE"
              }, 
              {
                "doc_count": 149, 
                "key": "Configuration RECO"
              }, 
              {
                "doc_count": 313, 
                "key": "Configuration SIM"
              }, 
              {
                "doc_count": 3, 
                "key": "Luminosity"
              }, 
              {
                "doc_count": 1722, 
                "key": "Trigger"
              }
            ], 
            "doc_count_error_upper_bound": 0, 
            "sum_other_doc_count": 0
          }
        }
      ], 
      "doc_count_error_upper_bound": 0, 
      "sum_other_doc_count": 0
    }, 
    "year": {
      "buckets": [
        {
          "doc_count": 2, 
          "key": "2009"
        }, 
        {
          "doc_count": 2, 
          "key": "2009-2012"
        }, 
        {
          "doc_count": 66, 
          "key": "2010"
        }, 
        {
          "doc_count": 826, 
          "key": "2010-2012"
        }, 
        {
          "doc_count": 1963, 
          "key": "2011"
        }, 
        {
          "doc_count": 4, 
          "key": "2011-2012"
        }, 
        {
          "doc_count": 1644, 
          "key": "2012"
        }
      ], 
      "doc_count_error_upper_bound": 0, 
      "sum_other_doc_count": 0
    }
  }, 
  "hits": {
    "hits": [
      {
        "created": "2017-12-20T14:54:51.052160+00:00", 
        "links": {
          "self": "http://opendata.cern.ch/api/records/atlas-higgs-machine-learning-challenge"
        }, 
        "metadata": {
          "$schema": "http://opendata.cern.ch/schema/records/docs-v1.0.0.json", 
          "author": "ATLAS Collaboration", 
          "body": {
            "content": "The dataset from the ATLAS Higgs Machine Learning Challenge has [been released](/search?page=1&size=20&q=atlas%20higgs%202014) on CERN Open Data Portal.\n\n[The Challenge](http://atlas.cern/updates/atlas-news/are-you-higgs-challenge), which ran from May to September 2014, was to develop an algorithm that improved the detection of the Higgs boson signal. The [specific sample](https://home.cern/about/updates/2013/11/atlas-sees-higgs-boson-decay-fermions) used simulated Higgs particles into two tau particles inside the ATLAS detector. The downloadable sample was provided for participants at the host platform on Kaggle\u2019s website. With almost 1,785 teams competing, the event was a huge success. Participants applied and developed cutting edge Machine Learning techniques, which have been shown to be better than existing traditional high-energy physics tools.\n\nThe dataset was removed at the end of the Challenge but due to high public demand, ATLAS as organiser of the event, has decided to house it in the CERN Open Data Portal where it will be available permanently. The 60MB zipped ASCII file can be decoded without a special software, and a few scripts are provided to help users get started. Detailed documentation for physicists and data scientists is also available. Thanks to the Digital Object Identifiers (DOIs) in CERN Open Data Portal, the dataset and accompanying material can be cited like any other paper.\n\nThe [Challenge\u2019s winner](https://atlas.cern/updates/atlas-news/machine-learning-wins-higgs-challenge) G\u00e1bor Melis and recipients of the Special High Energy Physics meets Machine Learning Award, Tianqi Chen and Tong He, will be visiting CERN to deliver talks on their winning algorithms on 19 May.", 
            "format": "md"
          }, 
          "collections": [
            {
              "experiment": "ATLAS"
            }, 
            {
              "primary": "News"
            }
          ], 
          "control_number": "atlas-higgs-machine-learning-challenge", 
          "created": "2015-02-17", 
          "experiment": "ATLAS", 
          "short_description": {
            "content": "The dataset from the ATLAS Higgs Machine Learning Challenge has been released on CERN Open Data Portal. The Challenge, which ran from May to September 2014, was to develop an algorithm that improved the detection of the Higgs boson signal."
          }, 
          "slug": "atlas-higgs-machine-learning-challenge", 
          "title": "ATLAS Higgs Machine Learning Challenge", 
          "type": {
            "primary": "News"
          }
        }, 
        "updated": "2018-04-04T16:05:15.003431+00:00"
      }, 
      {
        "created": "2017-12-20T14:54:49.281678+00:00", 
        "links": {
          "self": "http://opendata.cern.ch/api/records/cms-getting-started-2011"
        }, 
        "metadata": {
          "$schema": "http://opendata.cern.ch/schema/records/docs-v1.0.0.json", 
          "body": {
            "content": "1. [\"I have installed the CERN Virtual Machine: now what?\"](#vm)\n2. [\"OK! Where can I get the CMS data?\"](#data)\n3. [\"Nice! But how do I analyse these data?\"](#nice)\n4. [Option A: Analysing the primary dataset](#a)\n5. [Option B: Analysing reduced datasets](#b)\n6. [Performing your analysis on the PATtuples](#pat)\n\n## <a name=\"vm\">\"I have installed the CERN Virtual Machine: now what?\"</a>\n\nTo analyse CMS data collected in 2011 and 2012, you need **version 5.3.32** of [CMSSW](/glossary/CMSSW), supported only on **Scientific Linux 6**. If you are unfamiliar with Linux, take a look at [this short introduction to Linux](https://twiki.cern.ch/twiki/bin/view/CMSPublic/WorkBookBasicLinux) or try this interactive [command-line bootcamp](http://rik.smith-unna.com/command_line_bootcamp/). Once you have installed the [CMS-specific CERN Virtual Machine](/docs/cms-virtual-machine-2011), execute the following command in the terminal if you haven't done so before; it ensures that you have this version of CMSSW running:\n\n```shell\n$ cmsrel CMSSW_5_3_32\n```\n\nThen, make sure that you are always in the **CMSSW_5_3_32/src/** directory and that the CMS analysis environment is properly setup by entering the following commands in the terminal (you must do so every time you boot the VM before you can proceed):\n\n```shell\n$ cd CMSSW_5_3_32/src/\n$ cmsenv\n```\n\n## <a name=\"data\">\"OK! Where can I get the CMS data?\"</a>\n\nIt is best if we start off with a quick introduction to **[ROOT](http://root.cern.ch)**. ROOT is the framework used by several particle-physics experiments to work with the collected data. Although analysis is not itself performed within the ROOT GUI, it is instructive to understand how these files are structured and what data and collections they contain.\n\nThe primary data provided by CMS on the CERN Open Data Portal is in a format called \"[Analysis Object Data](/docs/cms-physics-objects-2011)\" or [AOD](/glossary/AOD) for short. These AOD files are prepared by piecing raw data collected by various sub-detectors of CMS and contain all the information that is needed for analysis. The files cannot be opened and understood as simple data tables but require ROOT in order to be read.\n\nSo, let's see what an AOD file looks like and take ROOT for a spin!\n\nMake sure that you are in the **CMSSW_5_3_32/src/** folder, and you have executed the `cmsenv` command in your terminal to launch the CMS analysis environment.\n\nYou can now open a CMS AOD file in ROOT. Let us open one of the files from the CERN Open Data Portal by entering the following command:\n\n```shell\n$ root root://eospublic.cern.ch//eos/opendata/cms/Run2011A/ElectronHad/AOD/12Oct2013-v1/20001/001F9231-F141-E311-8F76-003048F00942.root\n```\n\nYou will see the ROOT logo appear on screen. You can now open the ROOT GUI by entering:\n\n```shell\nTBrowser t\n```\n\nExcellent! You have successfully opened a CMS AOD file in ROOT. If this was the first time you've done so, pat yourself on the back. Now, to see what is inside this file, let us take a closer look at some collections of [physics objects](/docs/cms-physics-objects-2011).\n\nOn the left window of ROOT (see the screenshot below), double-click on the file name (<kbd>root://eospublic.cern.ch//eos/opendata/\u2026</kbd>). You should see a list of entries under <kbd>Events</kbd>, each corresponding to a collection of reconstructed data. We are interested in the collections containing information about reconstructed physics objects.\n\n<img src=\"/static/docs/getting-started-with-cms-2011-data/getting_started_with_cms_2011_data_1.png\"  width=\"70%\">\n\nLet us take a peek, for example, at the [electrons](/glossary/electron), which are found in <kbd>recoGsfElectrons_gsfElectrons__RECO</kbd>, as shown on the list of [physics objects](/docs/cms-physics-objects-2011). Look in there by double-clicking on that line and then double-clicking on <kbd>recoGsfElectrons_gsfElectrons__RECO.obj</kbd>. Here, you can have a look at various properties of this collection, such as the plot for the transverse momentum of the electrons: <kbd>recoGsfElectrons_gsfElectrons__RECO.obj.pt_</kbd>.\n\nYou can exit the ROOT browser through the GUI by clicking on <kbd>Browser</kbd> on the menu and then clicking on <kbd>Quit Root</kbd> or by entering <kbd>.q</kbd> in the terminal.\n\n## <a name=\"nice\">\"Nice! But how do I analyse these data?\"</a>\n\nIn AOD files, reconstructed [physics objects](/docs/cms-physics-objects-2011) are included without checking their \"quality\", i.e. in case of our electron collection that you opened in ROOT, without ensuring that the reconstructed object is really an electron. In order to analyse only the \"good quality\" data, you must apply some selection criteria.\n\nWith these criteria, you are in effect reducing the dataset, either in terms of the number of collisions events it contains or in terms of the information carried by each event. Following this, you run your analysis code on the reduced dataset.\n\nDepending on the nature of your analysis you _can_ run your analysis code directly on the AOD files themselves, if needed, performing the selections along the way. However, this can be resource-intensive and is done only for very specific usecases.\n\n**NOTE**: To analyse the full event content, the analysis job needs access to the \"[condition data](/glossary/tag)\", such as the jet-energy corrections. You can see how connections to the condition database are established in the [\"pattuples2011\" example](/record/233). For simpler analyses, where we use only physics objects needing no further data for corrections, you do not need to connect to the condition database. This is the case for the example for analysing the [primary datasets](/glossary/primary) below.\n\nYour final analysis is done using a software module called an \"analyzer\". If you have followed the validation step for the virtual machine setup, you have already produced and run a simple analyzer. You can specify your initial selection criteria within the analyzer to perform your analysis directly on the AOD files, or further elaborate the selections and other operations needed for analysing the reduced dataset. To learn more about configuring analyzers, follow [these instructions in the CMSSW WorkBook](https://twiki.cern.ch/twiki/bin/view/CMSPublic/WorkBookWriteFrameworkModule). Make sure, though, that you replace the release version (CMSSW_nnn) with the release that you are using, i.e. one that is compatible with the CMS open data.\n\nYou can also pass the selection criteria through the configuration file. This file activates existing tools within CMSSW in order to perform the desired selections. If you have followed the validation step for the virtual machine setup, you have already seen a configuration file, which is used to give the parameters to the <kbd>cmsRun</kbd> executable. You can see how this is done in our analysis example.\n\nWe will now take you through these steps through a couple of specially prepared example analyses.\n\n##  <a name=\"a\">Option A: Analysing the primary dataset</a>\n\nAs mentioned above, you do not typically perform an analysis directly on the AOD files. However, there may be cases when you can do so. Therefore, we have provided an example analysis to take you through the steps that you may need on the occassions that you want to analyse the AOD files directly. You can find the files and instructions in [this CMS analysis example](/record/5001).\n\n## <a name=\"b\">Option B: Analysing reduced datasets</a>\n\nWe start by applying selection cuts via the configuration file and reduce the AOD files into a format known as PATtuple. You can find more information about this data format (which gets its name from the CMS Physics Analysis Toolkit, or [PAT](/glossary/PAT)) on the [CMSSW PAT WorkBook](https://twiki.cern.ch/twiki/bin/view/CMSPublic/WorkBookPAT).\n\n**Important**: Be aware that the instructions in the WorkBook are in use in CMS currently and have been updated for more recent CMSSW releases. With the 2011 and 2012 data, you should always use the releases in the series of CMSSW_5_3 and not higher. Also note that more recent code does not work with older releases, so whenever you see <kbd>git cms-addpkg\u2026</kbd> in the instruction, it is likely that the code package this command adds does not work with the release you need. However, the material under the pages gives you a good introduction to [PAT](/glossary/PAT).\n\nCode as well as instructions for producing PATtuples from the CMS open data can be found in [this example](https://github.com/cms-opendata-analyses/pattuples2011). However, since it can take a dedicated computing cluster several days to run this step and reduce the several TB of AOD files to a few GB of PATtuples, we have provided you with the PATtuples in that GitHub repo, saving you quite a lot of time! So you can jump to the next step, below (\"Performing your analysis\u2026\"). Although you do not need to run this step, it is worth looking at [the configuration file]( https://github.com/cms-opendata-analyses/pattuples2011/blob/master/PAT_data_repo.py):\n\nYou can see that the line <kbd>removeAllPATObjectsBut(process, ['Muons','Electrons'])</kbd> removes all \"PATObjects\" but muon and electrons, which will be needed in the final analysis step of this example.\n\nNote also how only the validated runs are selected on lines:\n\n```python\nimport FWCore.ParameterSet.Config as cms\nimport FWCore.PythonUtilities.LumiList as LumiList\nmyLumis = LumiList.LumiList(filename='Cert_160404-180252_7TeV_ReRecoNov08_Collisions11_JSON.txt').getCMSSWString().split(',')\nprocess.source.lumisToProcess = cms.untracked.VLuminosityBlockRange()\nprocess.source.lumisToProcess.extend(myLumis)\n```\n\nThis selection must always be applied to any analysis on CMS open data, and to do so you must have the validation file downloaded to your local area.\n\nYou can also see the steps needed to use the condition data. First, as shown in the <kbd>README</kbd>, you have to set the symbolic links to the condition database for 2011 data.\n\n```shell\nln -sf /cvmfs/cms-opendata-conddb.cern.ch/FT_53_LV5_AN1_RUNA FT_53_LV5_AN1\nln -sf /cvmfs/cms-opendata-conddb.cern.ch/FT_53_LV5_AN1_RUNA.db FT_53_LV5_AN1_RUNA.db\n```\nMake sure the `cms-opendata-conddb.cern.ch` directory has actually expanded in your VM. One way of doing this is executing:\n\n```shell\nls -l\nls -l /cvmfs/\n```\n\nThen, the correct set of condition data are defined by mentioning the [Global Tag](/glossary/tag) on lines 46\u201348 in the file <kbd>PAT_data_repo.py</kbd>.\n\n```shell\n#globaltag\nprocess.GlobalTag.connect = cms.string('sqlite_file:/cvmfs/cms-opendata-conddb.cern.ch/FT_53_LV5_AN1_RUNA.db')\nprocess.GlobalTag.globaltag = 'FT_53_LV5_AN1::All'\n```\n\nSee detailed instructions for the use of condition data for different data-taking years in [the guide to the CMS condition database](/docs/cms-guide-for-condition-database).\n\n## <a name=\"pat\">Performing your analysis on the PATtuples</a>\n\nNow, as the intermediate PATtuple files have been produced for you, you can go directly to the next step, as described in [the analysis example](https://github.com/cms-opendata-analyses/OutreachExercise2011) and follow the instructions on that page.\n\nNote that even though these are [derived datasets](/glossary/derived), running the analysis code over the full data can take time. So if you want just give it a try, you can limit the number events or read only part of the files. Bear in mind that running on a low number of files will not give you a meaningful plot.\n\nYour analysis job is defined in <kbd>OutreachExercise2011/DecaysToLeptons/run/run.py</kbd>. The analysis code is in the files located in the <kbd>OutreachExercise2011/DecaysToLeptons/python</kbd> directory.\n\nThis example uses IPython, which gets configured and starts the job with the following command:\n\n```shell\nipython run.py\n```\n\nThat's it! Follow the rest of the instructions on the README and you have performed an analysis using data from CMS. Hope you enjoyed this exercise. Feel free to play around with the rest of the data and write your own analyzers and analysis code. (To exit IPython, enter <kbd>exit()</kbd>.)\n", 
            "format": "md"
          }, 
          "collections": [
            {
              "experiment": "CMS"
            }, 
            {
              "primary": "education"
            }, 
            {
              "year": "2011"
            }
          ], 
          "control_number": "cms-getting-started-2011", 
          "experiment": "CMS", 
          "short_description": {
            "content": "To analyse CMS data collected in 2011 and 2012, you need version 5.3.32 of CMSSW, supported only on Scientific Linux 6. If you are unfamiliar with Linux, take a look at this short introduction to Linux or try this..."
          }, 
          "slug": "cms-getting-started-2011", 
          "tags": [
            "Getting Started"
          ], 
          "title": "Getting Started with CMS 2011 Open Data", 
          "type": {
            "primary": "Documentation", 
            "secondary": [
              "Guide"
            ]
          }
        }, 
        "updated": "2018-04-04T16:05:15.269685+00:00"
      }, 
      {
        "created": "2017-12-20T14:54:49.566747+00:00", 
        "links": {
          "self": "http://opendata.cern.ch/api/records/cms-guide-trigger-system"
        }, 
        "metadata": {
          "$schema": "http://opendata.cern.ch/schema/records/docs-v1.0.0.json", 
          "body": {
            "content": "Physically, an event is the result of a single readout of the detector electronics and the signals that will (in general) have been generated by particles, tracks, energy deposits, present in a number of bunch crossings. The task of the online Trigger and Data Acquisition System (TriDAS) is to select, out of the millions of events recorded in the detector, the most interesting 100 or so per second, and then store them for further analysis. An event has to pass two independent sets of tests, or Trigger Levels, in order to qualify. The tests range from simple and of short duration (Level-1) to sophisticated ones requiring significantly more time to run (High Levels 2 and 3, called HLT). In the end, the HLT system creates RAW data events containing:\n- the detector data,\n- the level 1 trigger result\n- the result of the HLT selections (HLT trigger bits)\n- and some of the higher-level objects created during HLT processing.\n\nOriginal Source: [TriggerSystem](https://twiki.cern.ch/twiki/bin/view/CMSPublic/WorkBookCMSSWFramework#TriggerSystem \"TriggerSystem\")\n\n\n### The HLT\n\nThe HLT contains many trigger paths, each corresponding to a dedicated trigger (such as a prescaled single-electron trigger or a 3-jets-with-MET trigger). A path consists of several steps (software modules), each module performing a well-defined task such as unpacking (raw to digi), reconstruction of physics objects (electrons, muons, jets, MET, etc.), making intermediate decisions triggering more refined reconstructions in subsequent modules, or calculating the final decision for a trigger path. The CMSSW Framework/EDM ensures that if an intermediate filter decision on a trigger path is negative, the rest of the path is not executed (skipped) and the specific trigger is regarded as rejecting the event. In order to save CPU time, each reconstruction step is followed by a filter in order to avoid running time-consuming reco code if it is already clear it will not be needed.\nIn general it is expected that all HLT trigger paths are run, even if the event is already accepted by a path. In case this turns out to be too time-consuming, a truncated mode of HLT operations should be foreseen where the HLT short-circuits after the first accept (and after the triggers needed to classify the event for a primary data set and output stream are computed) and does not run the rest of the triggers. Presumably, the triggers not run online could be run in the offline reconstruction step to compute all trigger bits (for events written out) in order to get a complete trigger picture allowing trigger efficiency studies.\nEach HLT trigger path must be seeded by one or more L1 trigger bit seeds: the first filter module in each HLT path is looking for a suitable L1 seed (consisting of L1 bit[s] and L1 object[s]) as a starting point for that specific HLT trigger.\n\nOriginal Source: [SWGuideHighLevelTrigger](https://twiki.cern.ch/twiki/bin/view/CMSPublic/SWGuideHighLevelTrigger#How_it_works \"SWGuideHighLevelTrigger\")\n\n### Persistent Trigger Results Objects and Available Software\n\nTwo persistent HLT products are available:\n\n- TriggerResults: (subclassed from HLTGlobalStatus object), containing\nall the usual decision bits.\n\n    The TriggerResults product (available for events written to output)\nallows access to the configuration and trigger decisions, i.e.,\nall the usual \"trigger bits\", including:\n    1. Final decision of individual path and of full trigger table\n    2. Which triggers were run (some triggers may not run due to lack of a corresponding L1 seed or HLT running in truncated mode)\n    3. For each trigger rejecting the event, (the index of) which intermediate or final module instances on the path rejected the event\n    4. For each trigger encountering an error condition, (the index of)\nwhich module instances on the path encountered un-recoverable\nerrors.\n\n    The corresponding code can be found in [DataFormats/Common/interface/TriggerResults.h](https://github.com/cms-sw/cmssw/blob/CMSSW_5_3_X/DataFormats/Common/interface/TriggerResults.h \"TriggerResults.h\") and [DataFormats/Common/interface/HLTGlobalStatus.h](https://github.com/cms-sw/cmssw/blob/CMSSW_5_3_X/DataFormats/Common/interface/HLTGlobalStatus.h \"HLTGlobalStatus.h\")\n\n- TriggerEvent: summarising the \"L3\" trigger collections and \"L3\" filter decisions.\n\n    The corresponding code can be found in [DataFormats/HLTReco/interface/TriggerEvent.h](https://github.com/cms-sw/cmssw/blob/CMSSW_5_3_X/DataFormats/HLTReco/interface/TriggerEvent.h \"TriggerEvent.h\")\n\nAdditionally, the package [HLTrigger/HLTcore](htts://github.com/cms-sw/cmssw/tree/CMSSW_5_3_X/HLTrigger/HLTcore \"HLTrigger/HLTcore\") contains several\nanalyzers pulling out the trigger information.\nYou can use the corresponding analyzers directly - see their cfi files in\nthe python subdirectory - or copy relevant code pieces into your modules.\n\n- TriggerSummaryAnalyzerAOD: analyser printing the content of the TriggerEvent product\n- HLTEventAnalyzerAOD: analyser combining the information from TriggerResults and TriggerEvent products\n\nThe HLTEventAnalyzer plugin make use of the helper class [HLTConfigProvider](https://github.com/cms-sw/cmssw/blob/CMSSW_5_3_X/HLTrigger/HLTcore/interface/HLTConfigProvider.h \"HLTConfigProvider\") (also in [HLTrigger/HLTcore](https://github.com/cms-sw/cmssw/tree/CMSSW_5_3_X/HLTrigger/HLTcore \"HLTrigger/HLTcore\")), which extracts the HLT configuration (paths, modules) from the provenance.\n\nNote: this helper class must be initialised calling it's init(...)\nfrom the beginRun() method of your plugin using this helper class. The reason\nthat it has to be (re-)initialised in beginRun() is that the HLT\nconfiguration can (only) change at the boundary between runs.\n\nOriginal Source: [Persistent Trigger Results Objects](https://twiki.cern.ch/twiki/bin/view/CMSPublic/SWGuideHighLevelTrigger#Persistent_Trigger_Results_Objec \"Persistent Trigger Results Objects\")\n\nFind the software and usage instructions in\n\n- [Analysis code for extracting the trigger information from the CMS 2010 data](/record/5003)\n- [Analysis code for extracting the trigger information from the CMS 2011 data](/record/5004)\n", 
            "format": "md"
          }, 
          "collections": [
            {
              "experiment": "CMS"
            }, 
            {
              "primary": "documentation"
            }
          ], 
          "control_number": "cms-guide-trigger-system", 
          "experiment": "CMS", 
          "short_description": {
            "content": "Physically, an event is the result of a single readout of the detector electronics and the signals that will (in general) have been generated by particles, tracks, energy deposits, present in a number of bunch crossings. The task of the online Trigger and Data Acquisition System (TriDAS) is to select,..."
          }, 
          "slug": "cms-guide-trigger-system", 
          "tags": [
            "Getting Started"
          ], 
          "title": "Guide to the CMS Trigger System", 
          "type": {
            "primary": "Documentation", 
            "secondary": [
              "Guide"
            ]
          }
        }, 
        "updated": "2018-04-04T16:05:15.475060+00:00"
      }, 
      {
        "created": "2017-12-20T14:54:51.198498+00:00", 
        "links": {
          "self": "http://opendata.cern.ch/api/records/cms-releases-first-batch-of-high-level-lhc-open-data"
        }, 
        "metadata": {
          "$schema": "http://opendata.cern.ch/schema/records/docs-v1.0.0.json", 
          "author": "CMS Collaboration", 
          "body": {
            "content": "The Compact Muon Solenoid (CMS) Collaboration at CERN is excited to announce the public release of the first batch of high-level, analysable and open data from the Large Hadron Collider (LHC), recorded by the CMS detector. The datasets are available on the new CERN Open Data Portal and are being released into the public domain under the Creative Commons CC0 waiver, in keeping with CMS\u2019s commitment to data preservation and open data. [Read full release announcement](https://cms.cern/news/cms-releases-first-batch-high-level-lhc-open-data)", 
            "format": "md"
          }, 
          "collections": [
            {
              "experiment": "CMS"
            }, 
            {
              "primary": "News"
            }
          ], 
          "control_number": "cms-releases-first-batch-of-high-level-lhc-open-data", 
          "created": "2014-11-20", 
          "experiment": "CMS", 
          "short_description": {
            "content": "The Compact Muon Solenoid (CMS) Collaboration at CERN is excited to announce the public release of the first batch of high-level, analysable and open data from the Large Hadron Collider (LHC), recorded by the CMS detector."
          }, 
          "slug": "cms-releases-first-batch-of-high-level-lhc-open-data", 
          "title": "CMS releases first batch of high-level LHC open data", 
          "type": {
            "primary": "News"
          }
        }, 
        "updated": "2018-04-04T16:05:15.729730+00:00"
      }, 
      {
        "created": "2017-12-20T14:54:50.357847+00:00", 
        "links": {
          "self": "http://opendata.cern.ch/api/records/privacy-policy"
        }, 
        "metadata": {
          "$schema": "http://opendata.cern.ch/schema/records/docs-v1.0.0.json", 
          "body": {
            "content": "CERN does not track, collect or retain personal information from users of CERN Open Data Portal, except as otherwise provided herein. In order to enhance CERN Open Data Portal and monitor traffic, non-personal information such as IP addresses and cookies may be tracked and retained, as well as log files shared in aggregation with other community services. User provided information, like corrections of metadata or paper claims, will be integrated into the database without displaying its source and may shared with other services. CERN Open Data Portal will take all reasonable measures to protect the privacy of its users and to resist service interruptions, intentional attacks, or other events that may compromise the security of the CERN Open Data Portal website. If you have any questions about the CERN Open Data Portal privacy policy, please contact [opendata-support@cern.ch](mailto:opendata-support@cern.ch)", 
            "format": "md"
          }, 
          "collections": [], 
          "control_number": "privacy-policy", 
          "short_description": {
            "content": "CERN does not track, collect or retain personal information from users of CERN Open Data Portal, except as otherwise provided herein. In order to enhance CERN Open Data Portal and monitor traffic, non-personal information..."
          }, 
          "slug": "privacy-policy", 
          "title": "CERN Open Data Privacy Policy", 
          "type": {
            "primary": "Documentation"
          }
        }, 
        "updated": "2018-04-04T16:05:16.103633+00:00"
      }, 
      {
        "created": "2017-12-20T14:54:43.135680+00:00", 
        "links": {
          "self": "http://opendata.cern.ch/api/records/derived"
        }, 
        "metadata": {
          "$schema": "http://opendata.cern.ch/schema/records/glossary-term-v1.0.0.json", 
          "anchor": "derived", 
          "category": "specific", 
          "collections": [
            {
              "primary": "Terms"
            }
          ], 
          "control_number": "derived", 
          "definition": "Contains data that have been derived from the primary datasets. The data may be reduced in the sense that (a) only part of the information is kept or (b) only part of the events are selected.", 
          "experiment": [
            {
              "name": "CMS"
            }, 
            {
              "name": "ATLAS"
            }, 
            {
              "name": "ALICE"
            }, 
            {
              "name": "LHCb"
            }
          ], 
          "links": [
            {
              "text": "See all the \u201cDerived datasets\u201d collections on this portal", 
              "url": "http://opendata.cern.ch/search?ln=en&p=+%28collection%3AALICE-Derived-Datasets+OR+collection%3ACMS-Derived-Datasets+OR+collection%3AATLAS-Derived-Datasets+OR+collection%3ALHCb-Derived-Datasets%29&action_search="
            }
          ], 
          "see_also": [
            {
              "term": "primary"
            }, 
            {
              "term": "tag"
            }, 
            {
              "term": "condition"
            }
          ], 
          "term": [
            "Derived Dataset", 
            "Derived Datasets"
          ], 
          "type": {
            "primary": "Glossary"
          }
        }, 
        "updated": "2017-12-20T14:54:43.135687+00:00"
      }, 
      {
        "created": "2017-12-20T14:54:43.451824+00:00", 
        "links": {
          "self": "http://opendata.cern.ch/api/records/neutrino"
        }, 
        "metadata": {
          "$schema": "http://opendata.cern.ch/schema/records/glossary-term-v1.0.0.json", 
          "anchor": "neutrino", 
          "category": "generic", 
          "collections": [
            {
              "primary": "Terms"
            }
          ], 
          "control_number": "neutrino", 
          "definition": "An elementary particle belonging to the \u201clepton\u201d family of particles. Neutrinos and their anti-particles, anti-neutrinos, have no charge, although measurements show that they are not massless. Neutrinos rarely interact with matter: they can fly through lightyears of lead without coming to a stop. Therefore, they cannot be detected directly by the particle detectors at the LHC: their presence has to be inferred by detecting and measuring every other particle produced in the collisions and then applying conservation laws. Neutrinos are recorded as \u201cmissing transverse energy\u201d or MET, although MET could also be a sign of previously undiscovered, non-interacting particles.", 
          "links": [
            {
              "text": "Read more on Wikipedia", 
              "url": "https://en.wikipedia.org/wiki/Neutrino"
            }
          ], 
          "see_also": [
            {
              "term": "muon"
            }, 
            {
              "term": "neutrino"
            }
          ], 
          "term": [
            "Neutrino", 
            "Neutrinos"
          ], 
          "type": {
            "primary": "Glossary"
          }
        }, 
        "updated": "2017-12-20T14:54:43.451832+00:00"
      }, 
      {
        "created": "2017-12-20T14:54:43.482114+00:00", 
        "links": {
          "self": "http://opendata.cern.ch/api/records/jet"
        }, 
        "metadata": {
          "$schema": "http://opendata.cern.ch/schema/records/glossary-term-v1.0.0.json", 
          "anchor": "jet", 
          "category": "generic", 
          "collections": [
            {
              "primary": "Terms"
            }
          ], 
          "control_number": "jet", 
          "definition": "A jet is a shower of hadrons, which originate from a quark or a gluon, clustered together after being produced in particle collisions.", 
          "links": [
            {
              "text": "Read more on Wikipedia", 
              "url": "https://en.wikipedia.org/wiki/Jet_%28particle_physics%29"
            }
          ], 
          "see_also": [
            {
              "term": "proton"
            }, 
            {
              "term": "quark"
            }, 
            {
              "term": "hadron"
            }
          ], 
          "term": [
            "Particle jet", 
            "Particle jets"
          ], 
          "type": {
            "primary": "Glossary"
          }
        }, 
        "updated": "2017-12-20T14:54:43.482122+00:00"
      }, 
      {
        "created": "2017-12-20T14:54:43.515820+00:00", 
        "links": {
          "self": "http://opendata.cern.ch/api/records/PAT"
        }, 
        "metadata": {
          "$schema": "http://opendata.cern.ch/schema/records/glossary-term-v1.0.0.json", 
          "anchor": "PAT", 
          "category": "specific", 
          "collections": [
            {
              "primary": "Terms"
            }
          ], 
          "control_number": "PAT", 
          "definition": "Stands for Physics Analysis Toolkit, and provides easy access to algorithms developed by the CMS Physics Object Groups (POGs) in the framework of CMS Software (CMSSW), suitable for most CMS analyses.", 
          "experiment": [
            {
              "name": "CMS"
            }
          ], 
          "links": [
            {
              "text": "Read more in the CMS Software guide", 
              "url": "https://twiki.cern.ch/twiki/bin/view/CMSPublic/SWGuidePAT"
            }
          ], 
          "see_also": [
            {
              "term": "CMSSW"
            }
          ], 
          "term": [
            "PAT"
          ], 
          "type": {
            "primary": "Glossary"
          }
        }, 
        "updated": "2017-12-20T14:54:43.515832+00:00"
      }, 
      {
        "created": "2017-12-20T14:54:43.728477+00:00", 
        "links": {
          "self": "http://opendata.cern.ch/api/records/trigger"
        }, 
        "metadata": {
          "$schema": "http://opendata.cern.ch/schema/records/glossary-term-v1.0.0.json", 
          "anchor": "trigger", 
          "category": "generic", 
          "collections": [
            {
              "primary": "Terms"
            }
          ], 
          "control_number": "trigger", 
          "definition": "A system that determines which particle collisions are stored in primary datasets and which ones are discarded. The triggering is done first by a lower-level hardware-based trigger (L1) and then by the High-Level Trigger (HLT) on a computing farm.", 
          "links": [
            {
              "text": "Read more on Wikipedia", 
              "url": "https://en.wikipedia.org/wiki/Trigger_%28particle_physics%29"
            }
          ], 
          "see_also": [
            {
              "term": "generator"
            }, 
            {
              "term": "primary"
            }, 
            {
              "term": "derived"
            }
          ], 
          "term": [
            "Trigger", 
            "Triggers", 
            "L1", 
            "HLT"
          ], 
          "type": {
            "primary": "Glossary"
          }
        }, 
        "updated": "2017-12-20T14:54:43.728502+00:00"
      }
    ], 
    "total": 4613
  }, 
  "links": {
    "next": "http://opendata.cern.ch/api/records/?page=2&size=10", 
    "self": "http://opendata.cern.ch/api/records/?page=1&size=10"
  }
}